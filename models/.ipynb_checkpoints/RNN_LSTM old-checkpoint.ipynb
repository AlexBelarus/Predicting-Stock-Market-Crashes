{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Crashes in Financial Markets - RNN LSTM\n",
    "- Datasets: S&P500 (USA), Nikkei225 (Japan), SSE (Shanghai/China), HSI (Hong Kong), BSESN (India), SMI (Switzerland), BVSP (Brazil)\n",
    "- Model: RNN LSTM\n",
    "- Response variable: Crash within 1 / 3 / 6 months (0: no, 1: yes)\n",
    "- Crash definition: Drawdown in 99.5% quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from sklearn import metrics\n",
    "from datetime import datetime, timedelta\n",
    "from pylab import rcParams\n",
    "import os\n",
    "import importlib\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- data preparation -------------------- #\n",
    "os.chdir('/home/roman/Documents/Projects/Bubbles/models')\n",
    "from prepare_data_2 import DataLoader\n",
    "os.chdir('/home/roman/Documents/Projects/Bubbles/data')\n",
    "\n",
    "datasets_original = ['^GSPC.csv', '^N225.csv', 'SSE.csv','^HSI.csv', '^BSESN.csv',\\\n",
    "                     '^SSMI.csv', '^BVSP.csv']\n",
    "dataset_names = ['S&P 500', 'N225', 'SSE', 'HSI', 'BSESN', 'SMI', 'BVSP']\n",
    "data = DataLoader(datasets_original, dataset_names)\n",
    "\n",
    "# specify drawdown thresholds for crashes (determined in exploration.ipynb):\n",
    "crash_thresholds = [-0.091, -0.109, -0.120, -0.144, -0.166, -0.110, -0.233] # <-- Jacobsson\n",
    "#crash_thresholds = [-0.1053, -0.1495, -0.1706, -0.2334, -0.1563, -0.1492, -0.2264] # <-- Sornette\n",
    "df_combined, drawdowns, crashes = data.get_df_combined(crash_thresholds)\n",
    "\n",
    "months = 3               # <-- predict if crash n months ahead\n",
    "select_features = False  # <-- if True: 8 time windows for mean price change and vol year\n",
    "sequence = 5             # <-- number of days lookback as input(only if select_features=False)\n",
    "#additional_feat = False  # <-- if True: add mean price change and volatility for 4 time widnows over 252 days\n",
    "batch_size = 50          # <-- batch size needs to be specified to satisfy stateful=True\n",
    "vol = False             # <-- if True: include 10 day volatility for each day in sequence (only in prepare_data_2)\n",
    "dfs_xy = data.get_df_xy(months=months, sequence=sequence, df_combined=df_combined, crashes=crashes, \\\n",
    "                        select_features=select_features, vol=vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (50, 5, 50)               10400     \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (50, 50)                  20200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (50, 1)                   51        \n",
      "=================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -------------------- RNN LSTM model -------------------- #\n",
    "model_name = 'RNN LSTM'\n",
    "neurons = 50\n",
    "dropout = 0\n",
    "optimizer = 'adam'\n",
    "loss = 'mse'   # 'binary_crossentropy'\n",
    "inp_tsteps = sequence + 8 * additional_feat\n",
    "def rnn_lstm(inp_tsteps, inp_dim, neurons, dropout):\n",
    "    model = Sequential()\n",
    "    #model.add(LSTM(neurons, input_shape=(inp_tsteps, inp_dim), return_sequences=True))\n",
    "    model.add(LSTM(neurons, input_shape=(inp_tsteps, inp_dim), stateful=True, return_sequences=True))\n",
    "    #model.add(Dropout(dropout))\n",
    "    #model.add(LSTM(neurons, return_sequences=True))\n",
    "    model.add(LSTM(neurons, stateful=False, return_sequences=False))\n",
    "    #model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='linear'))   # 'sigmoid'\n",
    "    return model\n",
    "model = rnn_lstm(neurons=neurons, inp_tsteps=inp_tsteps, inp_dim=1, dropout=dropout)\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RNN LSTM - test data: S&P 500\n",
      "Epoch 1/5\n",
      "44400/44400 [==============================] - 14s 313us/step - loss: 0.1963 - acc: 0.9540\n",
      "Epoch 2/5\n",
      "44400/44400 [==============================] - 13s 285us/step - loss: 0.1852 - acc: 0.9546\n",
      "Epoch 3/5\n",
      "44400/44400 [==============================] - 13s 294us/step - loss: 0.1852 - acc: 0.9546\n",
      "Epoch 4/5\n",
      "44400/44400 [==============================] - 12s 281us/step - loss: 0.1853 - acc: 0.9546\n",
      "Epoch 5/5\n",
      "44400/44400 [==============================] - 12s 275us/step - loss: 0.1852 - acc: 0.9546\n",
      "Train RNN LSTM - test data: N225\n",
      "Epoch 1/5\n",
      "48450/48450 [==============================] - 16s 324us/step - loss: 0.1917 - acc: 0.9554\n",
      "Epoch 2/5\n",
      "48450/48450 [==============================] - 14s 284us/step - loss: 0.1816 - acc: 0.9558\n",
      "Epoch 3/5\n",
      "13700/48450 [=======>......................] - ETA: 10s - loss: 0.1770 - acc: 0.9572"
     ]
    }
   ],
   "source": [
    "# -------------------- Train and test RNN LSTM model -------------------- #\n",
    "epochs = 5\n",
    "#batch_size = 50\n",
    "y_train_all = []\n",
    "y_test_all = []\n",
    "y_pred_tr_all = []\n",
    "y_pred_t_all = []\n",
    "for test_data in dataset_names:\n",
    "    np_train, np_test = data.get_train_test(dfs_xy, dataset_names, test_data=test_data)\n",
    "    #tr_len = np_train.shape[0] - np_train.shape[0] % batch_size #### adjustment for statefulness\n",
    "    #t_len = np_test.shape[0] - np_test.shape[0] % batch_size #### adjustment for statefulness\n",
    "    x_train = np_train[:, 0:-1]             #### adjustment for statefulness\n",
    "    x_train = np.expand_dims(x_train, axis=2)\n",
    "    y_train = np_train[:, -1].astype(int)   #### adjustment for statefulness\n",
    "    y_train_all.append(y_train)\n",
    "    x_test = np_test[:, 0:-1]                #### adjustment for statefulness\n",
    "    x_test = np.expand_dims(x_test, axis=2)\n",
    "    y_test = np_test[:, -1].astype(int)        #### adjustment for statefulness\n",
    "    y_test_all.append(y_test)\n",
    "    print('Train ' + str(model_name) + ' - test data: ' + str(test_data))\n",
    "    model = rnn_lstm(neurons=neurons, inp_tsteps=inp_tsteps, inp_dim=1, dropout=dropout)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    y_pred_tr = model.predict(x_train, batch_size=batch_size)   #### batch_size adjustment for statefulness\n",
    "    y_pred_tr_all.append(y_pred_tr)\n",
    "    y_pred_t = model.predict(x_test, batch_size=batch_size)\n",
    "    y_pred_t_all.append(y_pred_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results - RNN LSTM:\n",
      "\n",
      "\n",
      "Predict crash in:         3 months\n",
      "Number of epochs:         5\n",
      "Sequence length:          5\n",
      "Number of nerueons/layer: 50\n",
      "Batch size:               50\n",
      "Optimizer:                adam\n",
      "Loss function:            binary_crossentropy\n",
      "\n",
      "\n",
      "Results for each train/test split:\n",
      "                S&P 500  N225   SSE   HSI  BSESN   SMI  BVSP\n",
      "precision_tr       0.10  0.12  0.09  0.07   0.06  0.07  0.10\n",
      "recall_tr          0.22  0.27  0.20  0.15   0.13  0.16  0.23\n",
      "accuracy_tr        0.88  0.88  0.87  0.87   0.87  0.87  0.88\n",
      "score_fbeta_tr     0.18  0.22  0.16  0.12   0.11  0.13  0.18\n",
      "precision_t        0.11  0.08  0.12  0.06   0.06  0.10  0.13\n",
      "recall_t           0.26  0.18  0.24  0.14   0.16  0.19  0.31\n",
      "accuracy_t         0.88  0.87  0.87  0.87   0.87  0.87  0.88\n",
      "score_fbeta_t      0.20  0.14  0.20  0.11   0.12  0.16  0.24\n",
      "\n",
      "\n",
      "Results average over all train/test splits:\n",
      "Number of features: 5; number of rows: 430500\n",
      "Positive train cases actual:        0.04\n",
      "Positive train cases predicted:     0.1\n",
      "Avg precision train (model/random): 0.09 / 0.04\n",
      "Avg recall train (model/random):    0.2 / 0.1\n",
      "Avg accuracy train (model/random):  0.87 / 0.86\n",
      "Score train fbeta:                  0.16\n",
      "Positive test cases actual:         0.04\n",
      "Positive test cases predicted:      0.1\n",
      "Avg precision test (model/random):  0.09 / 0.04\n",
      "Avg recall test (model/random):     0.21 / 0.1\n",
      "Avg accuracy test (model/random):   0.87 / 0.86\n",
      "Score test fbeta:                   0.17\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Evaluate results -------------------- #\n",
    "pct_pos = 0.10   # <-- tune: increase leads to higher recall, less precision\n",
    "beta = 2\n",
    "precision_tr = []\n",
    "recall_tr = []\n",
    "accuracy_tr = []\n",
    "precision_t = []\n",
    "recall_t = []\n",
    "accuracy_t = []\n",
    "y_pred_t_bin_all = []\n",
    "y_pred_tr_bin_all = []\n",
    "score_fbeta_tr = []\n",
    "score_fbeta_t = []\n",
    "for y_train, y_test, y_pred_tr, y_pred_t in zip(y_train_all, y_test_all, \\\n",
    "                                                y_pred_tr_all, y_pred_t_all):\n",
    "    y_pred_tr_bin = y_pred_tr > np.percentile(y_pred_tr, 100 * (1-pct_pos))\n",
    "    y_pred_tr_bin = y_pred_tr_bin.astype(int)\n",
    "    y_pred_tr_bin_all.append(y_pred_tr_bin)\n",
    "    precision_tr.append(metrics.precision_score(y_train, y_pred_tr_bin))\n",
    "    recall_tr.append(metrics.recall_score(y_train, y_pred_tr_bin))\n",
    "    accuracy_tr.append(metrics.accuracy_score(y_train, y_pred_tr_bin))\n",
    "    score_fbeta_tr.append(metrics.fbeta_score(y_train, y_pred_tr_bin, beta=beta))\n",
    "    y_pred_t_bin = y_pred_t > np.percentile(y_pred_t, 100 * (1-pct_pos))\n",
    "    y_pred_t_bin = y_pred_t_bin.astype(int)\n",
    "    y_pred_t_bin_all.append(y_pred_t_bin)\n",
    "    precision_t.append(metrics.precision_score(y_test, y_pred_t_bin))\n",
    "    recall_t.append(metrics.recall_score(y_test, y_pred_t_bin))\n",
    "    accuracy_t.append(metrics.accuracy_score(y_test, y_pred_t_bin))\n",
    "    score_fbeta_t.append(metrics.fbeta_score(y_test, y_pred_t_bin, beta=beta))\n",
    "\n",
    "y_tr_pred_pos = [sum(y_pred) / (len(y_pred)) for y_pred in y_pred_tr_bin_all]\n",
    "y_t_pred_pos = [sum(y_pred) / (len(y_pred)) for y_pred in y_pred_t_bin_all]\n",
    "d = {'precision_tr': np.round(precision_tr,2), \\\n",
    "     'recall_tr': np.round(recall_tr,2), 'accuracy_tr': np.round(accuracy_tr,2), \\\n",
    "     'score_fbeta_tr': np.round(score_fbeta_tr,2), \\\n",
    "     'precision_t': np.round(precision_t,2), \\\n",
    "     'recall_t': np.round(recall_t,2), 'accuracy_t': np.round(accuracy_t,2), \\\n",
    "     'score_fbeta_t': np.round(score_fbeta_t,2)}\n",
    "results = pd.DataFrame.from_dict(d, orient='index')\n",
    "results.columns = dataset_names\n",
    "print('Results - ' + model_name + ':')\n",
    "print('\\n')\n",
    "print('Predict crash in:         ' + str(months) + ' months')\n",
    "print('Number of epochs:         ' + str(epochs))\n",
    "print('Sequence length:          ' + str(sequence))\n",
    "print('Number of nerueons/layer: ' + str(neurons))\n",
    "print('Batch size:               ' + str(batch_size))\n",
    "print('Optimizer:                ' + str(optimizer))\n",
    "print('Loss function:            ' + str(loss))\n",
    "print('\\n')\n",
    "print('Results for each train/test split:')\n",
    "print(results)\n",
    "print('\\n')\n",
    "\n",
    "# score: square of loss, multiplied by wtd over precision, recall, accuracy\n",
    "avg_pr_tr = sum(precision_tr)/len(precision_tr)\n",
    "avg_re_tr = sum(recall_tr)/len(recall_tr)\n",
    "avg_ac_tr = sum(accuracy_tr)/len(accuracy_tr)\n",
    "avg_pr_t = sum(precision_t)/len(precision_t)\n",
    "avg_re_t = sum(recall_t)/len(recall_t)\n",
    "avg_ac_t = sum(accuracy_t)/len(accuracy_t)\n",
    "avg_score_fbeta_tr = sum(score_fbeta_tr) / len(score_fbeta_tr)\n",
    "avg_score_fbeta_t = sum(score_fbeta_t) / len(score_fbeta_t)\n",
    "\n",
    "# calculate precision, recall, accuracy for comparable random model\n",
    "sum_tr = 0\n",
    "sum_t = 0\n",
    "pos_tr = 0\n",
    "pos_t = 0\n",
    "sum_tr_pred = 0\n",
    "sum_t_pred = 0\n",
    "pos_tr_pred = 0\n",
    "pos_t_pred = 0\n",
    "for y_tr, y_t, y_tr_pr, y_t_pr in zip(y_train_all, y_test_all, y_pred_tr_bin_all, \\\n",
    "                y_pred_t_bin_all):\n",
    "    sum_tr += len(y_tr)\n",
    "    pos_tr += sum(y_tr)\n",
    "    sum_t += len(y_t)\n",
    "    pos_t += sum(y_t)\n",
    "    sum_tr_pred += len(y_tr_pr)\n",
    "    sum_t_pred += len(y_t_pr)\n",
    "    pos_tr_pred += sum(y_tr_pr)[0]\n",
    "    pos_t_pred += sum(y_t_pr)[0]\n",
    "y_train_pos_actual = pos_tr / sum_tr\n",
    "y_train_pos_pred = pos_tr_pred / sum_tr_pred\n",
    "rnd_TP = y_train_pos_pred * y_train_pos_actual\n",
    "rnd_FP = y_train_pos_pred * (1 - y_train_pos_actual)\n",
    "rnd_TN = (1 - y_train_pos_pred) * (1 - y_train_pos_actual)\n",
    "rnd_FN = (1 - y_train_pos_pred) * y_train_pos_actual\n",
    "rnd_pr_tr = rnd_TP / (rnd_TP+rnd_FP)\n",
    "rnd_re_tr = rnd_TP / (rnd_TP+rnd_FN)\n",
    "rnd_ac_tr = rnd_TP + rnd_TN\n",
    "y_test_pos_actual = pos_t / sum_t\n",
    "y_test_pos_pred = pos_t_pred / sum_t_pred\n",
    "rnd_TP = y_test_pos_pred * y_test_pos_actual\n",
    "rnd_FP = y_test_pos_pred * (1 - y_test_pos_actual)\n",
    "rnd_TN = (1 - y_test_pos_pred) * (1 - y_test_pos_actual)\n",
    "rnd_FN = (1 - y_test_pos_pred) * y_test_pos_actual\n",
    "rnd_pr_t = rnd_TP / (rnd_TP+rnd_FP)\n",
    "rnd_re_t = rnd_TP / (rnd_TP+rnd_FN)\n",
    "rnd_ac_t = rnd_TP + rnd_TN\n",
    "\n",
    "print('Results average over all train/test splits:')\n",
    "print('Number of features: ' + str(sequence) + '; number of rows: ' \\\n",
    "      + str(sum_tr + sum_t))\n",
    "print('Positive train cases actual:        '+ str(round(y_train_pos_actual, 2)))\n",
    "print('Positive train cases predicted:     '+ str(round(y_train_pos_pred, 2)))\n",
    "print('Avg precision train (model/random): '+ str(round(avg_pr_tr, 2))+' / '+str(round(rnd_pr_tr, 2)))\n",
    "print('Avg recall train (model/random):    '+ str(round(avg_re_tr, 2))+' / '+str(round(rnd_re_tr, 2)))\n",
    "print('Avg accuracy train (model/random):  '+ str(round(avg_ac_tr, 2))+' / '+str(round(rnd_ac_tr, 2)))\n",
    "print('Score train fbeta:                  '+ str(round(avg_score_fbeta_tr, 2)))\n",
    "print('Positive test cases actual:         '+ str(round(y_test_pos_actual, 2)))\n",
    "print('Positive test cases predicted:      '+ str(round(y_test_pos_pred, 2)))\n",
    "print('Avg precision test (model/random):  '+ str(round(avg_pr_t, 2))+' / '+str(round(rnd_pr_t, 2)))\n",
    "print('Avg recall test (model/random):     '+ str(round(avg_re_t, 2))+' / '+str(round(rnd_re_t, 2)))\n",
    "print('Avg accuracy test (model/random):   '+ str(round(avg_ac_t, 2))+' / '+str(round(rnd_ac_t, 2)))\n",
    "print('Score test fbeta:                   '+ str(round(avg_score_fbeta_t, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -------------------- Plot results -------------------- #\n",
    "test_data = 'S&P 500'\n",
    "i = [i for i, name in enumerate(dataset_names) if name == test_data][0]\n",
    "dfs_predict = data.split_results(df_combined, dfs_xy, dataset_names, test_data, \\\n",
    "            y_pred_t_bin_all[i], y_pred_tr_bin_all[i], y_train_all[i], y_test_all[i])\n",
    "df = dfs_predict[i]\n",
    "c = crashes[i]\n",
    "t_start = [datetime.strptime('1984-01-01', '%Y-%m-%d'), datetime.strptime('1994-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2007-01-01', '%Y-%m-%d')]\n",
    "t_end = [datetime.strptime('1991-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2002-01-01', '%Y-%m-%d'), datetime.strptime('2010-01-01', '%Y-%m-%d')]\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "for t1, t2 in zip(t_start, t_end):\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[2.5, 1, 1])\n",
    "    plt.subplot(gs[0])\n",
    "    y_start = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) < 0].index)\n",
    "    y_end = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) > 0].index)\n",
    "    crash_st = list(filter(lambda x: x > t1 and x < t2, c['crash_st']))\n",
    "    crash_end = list(filter(lambda x: x > t1 and x < t2, c['crash_end']))\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    df_norm = df['price'][t1:t2] / df['price'][t1:t2].max()\n",
    "    plt.plot(df_norm[t1:t2], color='blue') \n",
    "    plt.title(model_name + ' Testcase: ' + test_data + ' ' + str(t1.year) + '-' \\\n",
    "              + str(t2.year))\n",
    "    plt.legend(['price', 'downturn / crash'])\n",
    "    plt.xticks([])\n",
    "    plt.grid()     \n",
    "    plt.subplot(gs[1])\n",
    "    plt.plot(df.loc[t1:t2, 'vol'])\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['Volatility'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.xticks([])\n",
    "    plt.subplot(gs[2])\n",
    "    plt.plot(df['y'][t1:t2], color='black')\n",
    "    plt.plot(df['y_pred'][t1:t2].rolling(10).mean(), color='darkred', linewidth=0.8)\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['crash within 6m', 'crash predictor'])\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()  \n",
    "\n",
    "test_data = 'N225'\n",
    "i = [i for i, name in enumerate(dataset_names) if name == test_data][0]\n",
    "dfs_predict = data.split_results(df_combined, dfs_xy, dataset_names, test_data, \\\n",
    "            y_pred_t_bin_all[i], y_pred_tr_bin_all[i], y_train_all[i], y_test_all[i])\n",
    "df = dfs_predict[i]\n",
    "c = crashes[i]\n",
    "t_start = [datetime.strptime('1984-01-01', '%Y-%m-%d'), datetime.strptime('2005-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2013-01-01', '%Y-%m-%d')]\n",
    "t_end = [datetime.strptime('1992-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2012-01-01', '%Y-%m-%d'), datetime.strptime('2017-01-01', '%Y-%m-%d')]\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "for t1, t2 in zip(t_start, t_end):\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[2.5, 1, 1])\n",
    "    plt.subplot(gs[0])\n",
    "    y_start = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) < 0].index)\n",
    "    y_end = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) > 0].index)\n",
    "    crash_st = list(filter(lambda x: x > t1 and x < t2, c['crash_st']))\n",
    "    crash_end = list(filter(lambda x: x > t1 and x < t2, c['crash_end']))\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    df_norm = df['price'][t1:t2] / df['price'][t1:t2].max()\n",
    "    plt.plot(df_norm[t1:t2], color='blue') \n",
    "    plt.title(model_name + ' Testcase: ' + test_data + ' ' + str(t1.year) + '-' \\\n",
    "              + str(t2.year))\n",
    "    plt.legend(['price', 'downturn / crash'])\n",
    "    plt.xticks([])\n",
    "    plt.grid()     \n",
    "    plt.subplot(gs[1])\n",
    "    plt.plot(df.loc[t1:t2, 'vol'])\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['Volatility'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.xticks([])\n",
    "    plt.subplot(gs[2])\n",
    "    plt.plot(df['y'][t1:t2], color='black')\n",
    "    plt.plot(df['y_pred'][t1:t2].rolling(10).mean(), color='darkred', linewidth=0.8)\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['crash within 6m', 'crash predictor'])\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()      \n",
    " \n",
    "    \n",
    "test_data = 'SSE'\n",
    "i = [i for i, name in enumerate(dataset_names) if name == test_data][0]\n",
    "dfs_predict = data.split_results(df_combined, dfs_xy, dataset_names, test_data, \\\n",
    "            y_pred_t_bin_all[i], y_pred_tr_bin_all[i], y_train_all[i], y_test_all[i])\n",
    "df = dfs_predict[i]\n",
    "c = crashes[i]\n",
    "t_start = [df.index[0], datetime.strptime('2004-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2010-01-01', '%Y-%m-%d')]\n",
    "t_end = [datetime.strptime('2002-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2010-01-01', '%Y-%m-%d'), df.index[-1]]\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "for t1, t2 in zip(t_start, t_end):\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[2.5, 1, 1])\n",
    "    plt.subplot(gs[0])\n",
    "    y_start = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) < 0].index)\n",
    "    y_end = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) > 0].index)\n",
    "    crash_st = list(filter(lambda x: x > t1 and x < t2, c['crash_st']))\n",
    "    crash_end = list(filter(lambda x: x > t1 and x < t2, c['crash_end']))\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    df_norm = df['price'][t1:t2] / df['price'][t1:t2].max()\n",
    "    plt.plot(df_norm[t1:t2], color='blue') \n",
    "    plt.title(model_name + ' Testcase: ' + test_data + ' ' + str(t1.year) + '-' \\\n",
    "              + str(t2.year))\n",
    "    plt.legend(['price', 'downturn / crash'])\n",
    "    plt.xticks([])\n",
    "    plt.grid()     \n",
    "    plt.subplot(gs[1])\n",
    "    plt.plot(df.loc[t1:t2, 'vol'])\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['Volatility'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.xticks([])\n",
    "    plt.subplot(gs[2])\n",
    "    plt.plot(df['y'][t1:t2], color='black')\n",
    "    plt.plot(df['y_pred'][t1:t2].rolling(10).mean(), color='darkred', linewidth=0.8)\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['crash within 6m', 'crash predictor'])\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()  \n",
    "    \n",
    "test_data = 'HSI'\n",
    "i = [i for i, name in enumerate(dataset_names) if name == test_data][0]\n",
    "dfs_predict = data.split_results(df_combined, dfs_xy, dataset_names, test_data, \\\n",
    "            y_pred_t_bin_all[i], y_pred_tr_bin_all[i], y_train_all[i], y_test_all[i])\n",
    "df = dfs_predict[i]\n",
    "c = crashes[i]\n",
    "t_start = [df.index[0], datetime.strptime('1995-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2005-01-01', '%Y-%m-%d')]\n",
    "t_end = [datetime.strptime('1990-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('1999-01-01', '%Y-%m-%d'), datetime.strptime('2010-01-01', '%Y-%m-%d')]\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "for t1, t2 in zip(t_start, t_end):\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[2.5, 1, 1])\n",
    "    plt.subplot(gs[0])\n",
    "    y_start = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) < 0].index)\n",
    "    y_end = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) > 0].index)\n",
    "    crash_st = list(filter(lambda x: x > t1 and x < t2, c['crash_st']))\n",
    "    crash_end = list(filter(lambda x: x > t1 and x < t2, c['crash_end']))\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    df_norm = df['price'][t1:t2] / df['price'][t1:t2].max()\n",
    "    plt.plot(df_norm[t1:t2], color='blue') \n",
    "    plt.title(model_name + ' Testcase: ' + test_data + ' ' + str(t1.year) + '-' \\\n",
    "              + str(t2.year))\n",
    "    plt.legend(['price', 'downturn / crash'])\n",
    "    plt.xticks([])\n",
    "    plt.grid()     \n",
    "    plt.subplot(gs[1])\n",
    "    plt.plot(df.loc[t1:t2, 'vol'])\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['Volatility'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.xticks([])\n",
    "    plt.subplot(gs[2])\n",
    "    plt.plot(df['y'][t1:t2], color='black')\n",
    "    plt.plot(df['y_pred'][t1:t2].rolling(10).mean(), color='darkred', linewidth=0.8)\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['crash within 6m', 'crash predictor'])\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()  \n",
    "    \n",
    "test_data = 'BSESN'\n",
    "i = [i for i, name in enumerate(dataset_names) if name == test_data][0]\n",
    "dfs_predict = data.split_results(df_combined, dfs_xy, dataset_names, test_data, \\\n",
    "            y_pred_t_bin_all[i], y_pred_tr_bin_all[i], y_train_all[i], y_test_all[i])\n",
    "df = dfs_predict[i]\n",
    "c = crashes[i]\n",
    "t_start = [datetime.strptime('1995-01-01', '%Y-%m-%d'), datetime.strptime('2005-01-01', '%Y-%m-%d')]\n",
    "t_end = [datetime.strptime('2003-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2010-01-01', '%Y-%m-%d')]\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "for t1, t2 in zip(t_start, t_end):\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[2.5, 1, 1])\n",
    "    plt.subplot(gs[0])\n",
    "    y_start = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) < 0].index)\n",
    "    y_end = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) > 0].index)\n",
    "    crash_st = list(filter(lambda x: x > t1 and x < t2, c['crash_st']))\n",
    "    crash_end = list(filter(lambda x: x > t1 and x < t2, c['crash_end']))\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    df_norm = df['price'][t1:t2] / df['price'][t1:t2].max()\n",
    "    plt.plot(df_norm[t1:t2], color='blue') \n",
    "    plt.title(model_name + ' Testcase: ' + test_data + ' ' + str(t1.year) + '-' \\\n",
    "              + str(t2.year))\n",
    "    plt.legend(['price', 'downturn / crash'])\n",
    "    plt.xticks([])\n",
    "    plt.grid()     \n",
    "    plt.subplot(gs[1])\n",
    "    plt.plot(df.loc[t1:t2, 'vol'])\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['Volatility'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.xticks([])\n",
    "    plt.subplot(gs[2])\n",
    "    plt.plot(df['y'][t1:t2], color='black')\n",
    "    plt.plot(df['y_pred'][t1:t2].rolling(10).mean(), color='darkred', linewidth=0.8)\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['crash within 6m', 'crash predictor'])\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()  \n",
    "\n",
    "test_data = 'SMI'\n",
    "i = [i for i, name in enumerate(dataset_names) if name == test_data][0]\n",
    "dfs_predict = data.split_results(df_combined, dfs_xy, dataset_names, test_data, \\\n",
    "            y_pred_t_bin_all[i], y_pred_tr_bin_all[i], y_train_all[i], y_test_all[i])\n",
    "df = dfs_predict[i]\n",
    "c = crashes[i]\n",
    "t_start = [datetime.strptime('1994-01-01', '%Y-%m-%d'), datetime.strptime('2000-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2010-01-01', '%Y-%m-%d')]\n",
    "t_end = [datetime.strptime('2000-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2006-01-01', '%Y-%m-%d'), datetime.strptime('2016-01-01', '%Y-%m-%d')]\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "for t1, t2 in zip(t_start, t_end):\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[2.5, 1, 1])\n",
    "    plt.subplot(gs[0])\n",
    "    y_start = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) < 0].index)\n",
    "    y_end = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) > 0].index)\n",
    "    crash_st = list(filter(lambda x: x > t1 and x < t2, c['crash_st']))\n",
    "    crash_end = list(filter(lambda x: x > t1 and x < t2, c['crash_end']))\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    df_norm = df['price'][t1:t2] / df['price'][t1:t2].max()\n",
    "    plt.plot(df_norm[t1:t2], color='blue') \n",
    "    plt.title(model_name + ' Testcase: ' + test_data + ' ' + str(t1.year) + '-' \\\n",
    "              + str(t2.year))\n",
    "    plt.legend(['price', 'downturn / crash'])\n",
    "    plt.xticks([])\n",
    "    plt.grid()     \n",
    "    plt.subplot(gs[1])\n",
    "    plt.plot(df.loc[t1:t2, 'vol'])\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['Volatility'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.xticks([])\n",
    "    plt.subplot(gs[2])\n",
    "    plt.plot(df['y'][t1:t2], color='black')\n",
    "    plt.plot(df['y_pred'][t1:t2].rolling(10).mean(), color='darkred', linewidth=0.8)\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['crash within 6m', 'crash predictor'])\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()  \n",
    "\n",
    "test_data = 'BVSP'\n",
    "i = [i for i, name in enumerate(dataset_names) if name == test_data][0]\n",
    "dfs_predict = data.split_results(df_combined, dfs_xy, dataset_names, test_data, \\\n",
    "            y_pred_t_bin_all[i], y_pred_tr_bin_all[i], y_train_all[i], y_test_all[i])\n",
    "df = dfs_predict[i]\n",
    "c = crashes[i]\n",
    "t_start = [df.index[0], datetime.strptime('2004-01-01', '%Y-%m-%d')]\n",
    "t_end = [datetime.strptime('2000-01-01', '%Y-%m-%d'), \\\n",
    "           datetime.strptime('2010-01-01', '%Y-%m-%d')]\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "for t1, t2 in zip(t_start, t_end):\n",
    "    gs = gridspec.GridSpec(3, 1, height_ratios=[2.5, 1, 1])\n",
    "    plt.subplot(gs[0])\n",
    "    y_start = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) < 0].index)\n",
    "    y_end = list(df[t1:t2][df.loc[t1:t2, 'y'].diff(-1) > 0].index)\n",
    "    crash_st = list(filter(lambda x: x > t1 and x < t2, c['crash_st']))\n",
    "    crash_end = list(filter(lambda x: x > t1 and x < t2, c['crash_end']))\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    df_norm = df['price'][t1:t2] / df['price'][t1:t2].max()\n",
    "    plt.plot(df_norm[t1:t2], color='blue') \n",
    "    plt.title(model_name + ' Testcase: ' + test_data + ' ' + str(t1.year) + '-' \\\n",
    "              + str(t2.year))\n",
    "    plt.legend(['price', 'downturn / crash'])\n",
    "    plt.xticks([])\n",
    "    plt.grid()     \n",
    "    plt.subplot(gs[1])\n",
    "    plt.plot(df.loc[t1:t2, 'vol'])\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['Volatility'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.xticks([])\n",
    "    plt.subplot(gs[2])\n",
    "    plt.plot(df['y'][t1:t2], color='black')\n",
    "    plt.plot(df['y_pred'][t1:t2].rolling(10).mean(), color='darkred', linewidth=0.8)\n",
    "    [plt.axvspan(x1, x2, alpha=0.2, color='red') for x1, x2 in zip(y_start, y_end)]\n",
    "    [plt.axvspan(x1, x2, alpha=0.5, color='red') for x1, x2 in zip(crash_st, crash_end)]\n",
    "    plt.legend(['crash within 6m', 'crash predictor'])\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
